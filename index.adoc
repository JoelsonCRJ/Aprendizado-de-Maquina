:source-highlighter: pygments 	
:imagesdir: ./01

:stem:

= PGEE-5574: Tópicos Especiais em Processamento de Sinais: Aprendizado de Máquina
Joelson Rocha <JoelsonRocha.Eng@gmail.com>
:toc: left

== Introdução

Este espaço tem como objetivo documentar as atividades propostas pelo Professor Dr. Patrick Marques Ciarellix durante a disciplina de Tópicos Especiais em Processamento de Sinais: Aprendizado de Máquina. Aqui são compartilhados enunciados de questões, bem como a solução comentada e implementada.

Os códigos foram desenvolvidos na linguagem de programação Python, com auxílio das bibliotecas Numpy e Pandas, e estão disponíveis ao final da resolução de cada questão.

== Lista 1  ==

=== Pré-processamento de dados ===
=== Questão 1 ===
A base de dados link:01/nebulosa.txt[Nebulosa] está contaminada com ruídos,
redundâncias, dados incompletos (substituídos pelo valor -100), inconsistências e
outliers. Para esta base:

a) Obtenha os resultados da classificação (métrica acurácia) usando a técnica do vizinho
mais próximo (NN) e Rocchio. Utilize a distância Euclidiana e a base de dados crua, sem
pré-processamento. Use o conjunto de 143 amostras  para link:01/nebulosa_train.txt[treino] e o de 28 amostras
para link:01/nebulosa_test.txt[teste]. Remova as amostras com dados incompletos.

b)Realize um pré-processamento sobre os dados de forma a reduzir os ruídos, as
redundâncias, inconsistências, outliers e a interferência dos dados incompletos.
Obtenha os resultados da classificação usando a técnica do vizinho mais próximo (NN)
e Rocchio usando a distância Euclidiana e a mesma divisão dos dados.

c) Compare os resultados obtidos em a) e b). Qual deles retornou o melhor resultado?
Por quê?

=== Solução 1.a ===  

Inicialmente, observou-se que os dois primeiros atributos, ID e nome do usuário representado numericamente, não eram importantes para a tarefa de  classificação.  Sendo assim, fez-se o descarte dos mesmos. Observou-se também, que os atributos 6 e 7, data de nascimento e idade da pessoa, continham a mesma informação, dessa forma, fez-se a remorção do atributo idade.
Pede-se para realizar a remorção de amostras com dados incompletos,ou seja, que apresentam o valor _"-100"_  em algum de seus atributos.

*_Vizinho mais próximo (NN):_*

A técnica do vizinho mais próximo se baseia na distância euclidiana entre a amostra de teste e todas as amostras do conjunto de treino, ou seja, é necessário calcular todas as distâncias e atualizar o rótulo da amostra de teste com o rótulo da amostra do conjunto de treino que se encontra mais próxima. 

A listagem abaixo ilustra como foi feita a implementação do algoritmo NN, utilizando a distância euclidiana entre os atributos das amostras. Para cada amostra de teste é gerado um vetor de distâncias, sendo possível encontrar a distância mínima e seu respectivo endereço. Dessa forma, atualiza-se o rótulo da amostra de teste com o rótulo da amostra de treinamento mais próxima.

.NN
[source,python]
----
def NN(trainning_array,test_array,labels_column):
    labels=[]
    for i in range(0,test_array.shape[0]):
        distances=np.zeros((len(trainning_array)))
        for z in range(0,trainning_array.shape[0]):
            distances[z]=np.sqrt(np.sum(np.power(np.subtract(test_array[i,0:labels_column],trainning_array[z,0:labels_column]),2)))
        #min[0] = index of min value and min[1] is the min value
        distances = list(distances)
        min_index = distances.index(min(distances))
        labels.append(trainning_array[min_index,labels_column])
    return np.array(labels)
----

Após sua execução, essa função retorna um vetor com os novos rótulos encontrados para cada amostra de teste, para que dessa forma seja calculada a acurácia de tal método, comparando o vetor de rótulos corretos com os novos gerados pelo classificador NN. A função para o cálculo da acurácia segue abaixo.

.accuracy
[source,python]
----
def accuracy(labels,prediction):
    count=0
    for i in range(0,labels.shape[0]):
        if(labels[i]==prediction[i]):
            count=count+1
    return (count/len(labels))*100
----

*_Rocchio:_*

Esta técnica também se baseia na distância Euclidiana, porém medida entre a amostra de teste e os centróides gerados pelas amostras de treino de mesmo rótulo ( à partir da média de seus atributos).

A função _split_by_labels_  recebe as amostras de treinamento e as separa em 3 grupos de acordo com seus rótulos.

.split_by_labels
[source,python]
----
def split_by_labels(array):
    L1=array[array[:,array.shape[1]-1]==1]
    L2=array[array[:,array.shape[1]-1]==2]
    L3=array[array[:,array.shape[1]-1]==3]
    return L1,L2,L3
----

Em seguida, utilizou-se a função _sample_centrois_ para calcular os centroides de cada rótulo de forma individual à partir da média de cada um dos atributos das amostras de mesmo rótulo.

.sample_centroids
[source,python]
----
def sample_centroids(data):
    m=np.empty((data.shape[1] -1),dtype=float)
    for i in range (0,data.shape[1]-1):
        m[i]=np.mean(data[:,i],dtype=np.float64)
    return m
----

Uma vez tendo as coordenadas de cada um dos centroids, implementou-se a função _Rocchio_ , que calcula a distância de cada amostra de teste e atualiza seu rótulo com o centróid que está mais próximo dela.

.Rocchio
[source,python]
----
def rocchio(data,Center_1,Center_2,Center_3):
    labels=[]
    euclidian=np.empty((3),dtype = float)
    for i in range(0,data.shape[0]):
        for j in range(0,(data.shape[1]-1)):
            euclidian[0]=np.sqrt(np.power(np.sum(Center_1[j]-data[i,j]),2))
            euclidian[1]=np.sqrt(np.power(np.sum(Center_2[j]-data[i,j]),2))
            euclidian[2]=np.sqrt(np.power(np.sum(Center_3[j]-data[i,j]),2))
            
        if(np.min(euclidian)==euclidian[0]):
            labels.append(1)
        elif(np.min(euclidian)==euclidian[1]):
            labels.append(2)
        else:
            labels.append(3)
    return labels
----

Ao final, calculou-se a acurácia deste algorítmo e obteve-se o seguinte resultado, exposto na Tabela 1:

.Resultados NN e Rocchio: questão a.
[%header,cols=2*] 
|===
|Algoritmo
|Acurácia

|NN
|76.00%

|Rocchio
|96.00%
|===

=== Solução 1.b ===

Para esta questão utilizou-se o arredondamento dos dados como solução para o ruído, transformando números decimais em números inteiros. Dando continuidade ao pre-processamento de dados que foi feito na letra a, utilizou-se o artifício de substituição de valores fora da faixa (outliers) pela moda do atributo no qual o mesmo se encontra, implementação realizada na função _replace_by_mode_. 

.replace_by_mode
[source,python]
----
def replace_by_mode(array):
    for i in range(0,array.shape[1]-1):
        while (i==0):
            index = np.where((array[:,i]<1) | (array[:,i]>3)) #get indexes
            index = index[0] #array of indexes
            m=stats.mode(array[:,i],axis=None) #calculating the mode of this atribute
            for j in range(0,len(index)):
                array[index[j],i]=int(m[0])
            i=1
        
        index = np.where((array[:,i]<1) | (array[:,i]>4))
        index = index[0]
        m=stats.mode(array[:,i],axis=None)
        for j in range(0,len(index)):
            array[index[j],i]=int(m[0])
    return array
----

Os resultados obtidos com esse tipo de abordagem estão expostos na Tabela 2:


.Resultados NN e Rocchio: questão b
[%header,cols=2*] 
|===
|Algoritmo
|Acurácia

|NN
|72.00%

|Rocchio
|100.00%
|===

=== Solução 1.c  ===

Comparando as Tabelas 1 e 2, nota-se para essa base de dados, o algoritmo Rocchio teve uma acurácia melhor do que o NN nos dois casos. O pre-processamento dos dados conseguiu otimizar a tarefa de classificação para o algoritmo Rocchio, trazendo um resultado de 100%, após a detecção e substituição dos outiliers e a eliminação do ruído.

*Arquivos*

|| link:01/01.py[01.py]  || link:01/functions01.py[functions01.py] ||

=== Questão 2 ===
Dada a base de dados Breast Cancer Wisconsin (Diagnostic) (baixar em
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)):

a) Obtenha a acurácia de classificação quando usando o classificador vizinho mais
próximo (NN) (utilize a distância Euclidiana). Use os dados do arquivo wdbc.data,
sendo as primeiras 300 amostras para treino e as demais para teste. Antes, repare os
atributos da base de dados e a posição dos rótulos. Quais atributos você pode eliminar
da base de dados antes do experimento? Por quê?

b) Aplique o PCA sobre os dados de treino e selecione o número de componentes até eles
corresponderem a 90% da informação de variância dos dados (conforme mostrado nos
slides). Quantos componentes foram selecionados? Calcule a nova acurácia do NN
usando as componentes selecionadas. O resultado alterou de forma significativa em
relação ao obtido em a)? Qual foi a vantagem observada usando PCA?

c) Outra técnica para redução de dimensionalidade, mas de forma supervisionada, é o
Discriminante Linear de Fisher (para duas classes) e a sua versão multiclasse. Quando
aplicado este método o tamanho do vetor de características é reduzido para C-1, onde
C é o número de classes do problema. Seguindo os slides de
http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture8.pdf (há um exemplo
no meio), obtenha os novos dados após a aplicação de Fisher sobre os dados de treinoe obtenha a acurácia do NN sobre o conjunto de teste. Quais as vantagens desta abordagem sobre o PCA?

=== Solução 2.a  ===

Foram excluídas as duas primeiras colunas, pois elas dizem respeito ao ID e ao diagnóstico, variável alvo. Dessa forma,  se dividiu a base de dados conforme o enunciado e aplicou-se o algorítmo NN para a tarefa de classificação. Obtendo acurácia de *88.48%*.

=== Solução 2.b  ===

Para essa questão utilizou-se o cálculo da matriz de coeficientes de correlação, dessa forma o primeiro algoritmo implementado foi o para calcular a matriz stem:[\hat{X}], descrito abaixo:

[stem] 
++++ 
\hat{X}_{i,j}=\frac{X_{i,j}-\mu_{j}}{\sigma_{j}}
++++

Em seguida, se calculou a matriz de coeficientes de correlação stem:[C], com stem:[N=300]:

[stem]
++++
C=\frac{1}{(N-1)}\hat{X}^{T}\hat{X}
++++

O próximo passo foi extrair os autovalores e autovetores da matriz stem:[C]. Para tal, utilizou-se a função https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.eigh.html[numpy.linalg.eigh]. Para se encontrar o número de componentes que correspondem a *90%* da informação da variância dos dados, criou-se a função _reduce_percent_ , que soma os autovalores de forma decrescente até que o somatório relativo seja equivalente à condição de parada fornecida  pelo usuário (*90%*).

.reduce_percent
[source,python]
----
def reduce_percent(array,stop_condititon):
    total=0
    full=np.sum(array)
    for i in range(array.size-1,1,-1):
        total=total+(total+array[i])/full
        #print(total)
        if(total >= stop_condititon):
            return (total,abs(i-array.size))
----

Para essa base de dados, foram encontradas 5 componentes principais. Dessa forma, cria-se uma matriz com os autovetores associados aos 5 maiores autovalores encontrados (stem:[V_{30x5}]). Para se encontrar a nova matriz de amostras de treinamento com as 5 componentes principais,basta multiplicar a matriz stem:[\hat{X}_{300x30}] pela matriz stem:[V_{30x5}], resultando na matriz stem:[Mtr_{300x5}].

Para se encontrar a matriz de teste com as 5 principais componentes, basta se repetir o processo, porém utilizando o stem:[\mu] e stem:[\sigma] dos atributos da base de dados de treino.

A acurácia para a tarefa de classificação através do algorítmo NN com o conjunto de dados novo teve o valor de *72.12%*, uma diferença de *16.43%* do resultado para o conjunto de dados original.É importante ressaltar que o algoritmo PCA reduziu a dimensionalidade desse conjunto de dados em aproximadamente *6 vezes*, mantendo parte da informação contida nos dados originais (reduzindo-a em apenas *10%*), e tendo um resultado satisfatório.

*Arquivos*

|| link:01/02a.py[02a.py]  || link:01/02b.py[02b.py] || link:01/functions02.py[functions02.py] ||
 
=== Regressão Linear ===

=== Questão 3 ===
Para a base de dados link:01/Runner_num.txt[Runner] obtenha:

a) A equação linear que se ajusta aos dados e a RMSE;

b) Predizer o resultado para o ano de 2020;

c) Utilize o teste de hipótese de Kendall para verificar se existe dependência entre os atributos. Realize o teste para 5% e 1% de nível de significância. Informe os resultados;

d) Calcule o coeficiente de correlação entre os dados e realize o teste de hipótese de Pearson para 5% e 1% de nível de significância (teste bilateral). Informe os resultados.

=== Solução 3.a  ===

Inicialmente observou-se que a distribuição dos dados se assemelha a uma reta, polinômio de primeiro grau, conforme a Figura 1:


.Tempo dos corredores dos 100 metros livres das olimpíadas
image::03a_1.svg[a,600,opts=inline]

Dessa forma, realizou-se a regressão linear de primeiro grau para a base de dados. Os cálculos para encontrar os coefienteces stem:[w_{0}] e stem:[w_{1}] seguem abaixo:

[stem]
++++
w_{1} = \frac{\bar{xt}-\bar{x}\bar{t}}{\bar{x^{2}}-(\bar{x})^{2}}
++++

[stem]
++++
w_{0}=\bar{t}-w_{1}\bar{x}
++++

A equação de regressão linear que se ajusta ao conjunto de dados tem a seguinte forma e está ilustrada na Figura 2:

[stem]
++++
f=35.56 -0.012x
++++

.Equação de regressão linear stem:[f=35.56 -0.012x]
image::03a_2.svg[a,600,opts=inline]

Para calcular o erro RMSE utilizou-se a seguinte equação:

[stem]
++++
RMSE = \sqrt{\frac{1}{N}\sum_{k=1}^{N} (t_{k}-f(x_{k}))^{2}}
++++

O resultado obtido foi : RMSE = *22.13%*

=== Solução 3.b ===

Para se predizer o resultado para o ano de 2020, simplesmente se substituiu o valor 2020 na equação:

[stem]
++++
f(2020)=35.56 -0.012(2020)
++++

O resultado obtido foi: f(2020) = 9.53 s

=== Solução 3.c ===

Para o cálculo do stem:[\tau], utiliza-se a seguinte fórmula:

[stem]

++++
\tau = \frac{\sum_{i=2}^{N} \sum_{j=1}^{i-1} sgn(x_{j}-x_{i})sgn(y_{j}-y_{i})}{\sqrt{n_{x}}\sqrt{n_{y}}}
++++

Tal fórmula foi implementada com o auxílio da função  https://docs.python.org/2/library/itertools.html[itertools] para  criar uma lista com combinações entre os índices dos dois arrays. Em seguida criou-se a função _get_tal_ para calcular o stem:[\tau] .

.get_tal
[source,python]
----
def get_tal(a1,a2,combinations):
    positivos = 0
    negativos = 0
    matrix = 0
    for i in range(combinations.shape[0]):
        matrix=(a1[combinations[i][0]]-a1[combinations[i][1]])*(a2[combinations[i][0]]-a2[combinations[i][1]])
        if(matrix>0):
            positivos = positivos +1
        else:
            negativos=negativos+1
   
    N=(a1.size*(a1.size-1))/2

    tal = (positivos-negativos)/(N)
    return tal
----

O valor de stem:[\tau] calculado foi de : stem:[|\tau|] = 0.87. Pela tabela de distribuição de  T de Student: stem:[z_{1-\frac{0.05}{2}}=1.96] e stem:[z_{1-\frac{0.01}{2}}= 2.33]. Substituindo seus valores na equação para o teste de hipótese dos coeficientes de correlação de Kendall, tem-se:

Para 5%:
[stem]
++++
|\tau| > 1.96 \sqrt{\frac{2(2N+5)}{9N(N-1)}} 

\rightarrow 0.87 > 0.26
++++

Portanto, a hipótese nula foi rejeitada para 5% e existe a possibilidade de haver dependência entre x e y com 95% de significância.

Já para 1%:

[stem]
++++
|\tau| > 2.33 \sqrt{\frac{2(2N+5)}{9N(N-1)}} 

\rightarrow 0.87 > 0.31
++++

Portanto, a hipótese nula foi rejeitada para 1% e existe a possibilidade de haver dependência entre x e y com 99% de significância.


=== Solução 3.d ===

Para realizar o teste de hipótese pelo coeficiente de correlação de Pearson, primeiramente foi obtido o valor P, coeficietne de correlação entre dois atributos, utilizando-se a seguinte fórmula:

[stem]
++++
P=\frac{cov(x,y)}{\sqrt(var(x))\sqrt(var(y))} \rightarrow P = -0.91
++++

Nota-se que o valor de P se encontra próximo ao valor -1, o que significa que a correlação linear é quase perfeita negativa entre x e y.

Em seguida, calcula-se a estatística do teste:

[stem]
++++
t_{0}=\frac{\hat{p}\sqrt{N-2}}{\sqrt(1-\hat{p}^{2})} \rightarrow t_{0}= -11.26
++++

Substituindo seu valor e os valores de stem:[t_{\frac{0.05}{2},N-2} =2.052] e stem:[t_{\frac{0.01}{2},N-2} =2.771] fórmula para hipótese nula rejeita, tem-se:

Para 5%:

[stem]
++++
|t_{0}|>2.052 \rightarrow 11.26 > 2.052
++++

A hipótese nula foi rejeitada para 5%, portanto há confiabilidade de 95%.

Para 1%:

[stem]
++++
|t_{0}|>2.771 \rightarrow 11.26 > 2.771
++++

A hipótese nula foi rejeitada para 5%, portanto há confiabilidade de 99%.