:source-highlighter: pygments 	
:imagesdir: ./figs

:stem:

= PGEE-5574: Tópicos Especiais em Processamento de Sinais: Aprendizado de Máquina
Joelson Rocha <JoelsonRocha.Eng@gmail.com>
:toc: left

== Introdução

Este espaço tem como objetivo documentar as atividades propostas pelo Professor Dr. Patrick Marques Ciarellix durante a disciplina de Tópicos Especiais em Processamento de Sinais: Aprendizado de Máquina. Aqui são compartilhados enunciados de questões, bem como a solução comentada e implementada.

Os códigos foram desenvolvidos na linguagem de programação Python, com auxílio das bibliotecas Numpy e Pandas, e estão disponíveis ao final da resolução de cada questão.

== Lista 1  ==

=== Pré-processamento de dados ===
=== Questão 1 ===
A base de dados link:01/nebulosa.txt[Nebulosa] está contaminada com ruídos,
redundâncias, dados incompletos (substituídos pelo valor -100), inconsistências e
outliers. Para esta base:

a) Obtenha os resultados da classificação (métrica acurácia) usando a técnica do vizinho
mais próximo (NN) e Rocchio. Utilize a distância Euclidiana e a base de dados crua, sem
pré-processamento. Use o conjunto de 143 amostras  para link:01/nebulosa_train.txt[treino] e o de 28 amostras
para link:01/nebulosa_test.txt[teste]. Remova as amostras com dados incompletos.

b)Realize um pré-processamento sobre os dados de forma a reduzir os ruídos, as
redundâncias, inconsistências, outliers e a interferência dos dados incompletos.
Obtenha os resultados da classificação usando a técnica do vizinho mais próximo (NN)
e Rocchio usando a distância Euclidiana e a mesma divisão dos dados.

c) Compare os resultados obtidos em a) e b). Qual deles retornou o melhor resultado?
Por quê?

=== Solução 1.a ===  

Inicialmente, observou-se que os dois primeiros atributos, ID e nome do usuário representado numericamente, não eram importantes para a tarefa de  classificação.  Sendo assim, fez-se o descarte dos mesmos. Observou-se também, que os atributos 6 e 7, data de nascimento e idade da pessoa, continham a mesma informação, dessa forma, fez-se a remorção do atributo idade.
Pede-se para realizar a remorção de amostras com dados incompletos,ou seja, que apresentam o valor _"-100"_  em algum de seus atributos.

*_Vizinho mais próximo (NN):_*

A técnica do vizinho mais próximo se baseia na distância euclidiana entre a amostra de teste e todas as amostras do conjunto de treino, ou seja, é necessário calcular todas as distâncias e atualizar o rótulo da amostra de teste com o rótulo da amostra do conjunto de treino que se encontra mais próxima. 

A listagem abaixo ilustra como foi feita a implementação do algoritmo NN, utilizando a distância euclidiana entre os atributos das amostras. Para cada amostra de teste é gerado um vetor de distâncias, sendo possível encontrar a distância mínima e seu respectivo endereço. Dessa forma, atualiza-se o rótulo da amostra de teste com o rótulo da amostra de treinamento mais próxima.

.NN
[source,python]
----
def NN(trainning_array,test_array,labels_column):
    labels=[]
    for i in range(0,test_array.shape[0]):
        distances=np.zeros((len(trainning_array)))
        for z in range(0,trainning_array.shape[0]):
            distances[z]=np.sqrt(np.sum(np.power(np.subtract(test_array[i,0:labels_column],trainning_array[z,0:labels_column]),2)))
        #min[0] = index of min value and min[1] is the min value
        distances = list(distances)
        min_index = distances.index(min(distances))
        labels.append(trainning_array[min_index,labels_column])
    return np.array(labels)
----

Após sua execução, essa função retorna um vetor com os novos rótulos encontrados para cada amostra de teste, para que dessa forma seja calculada a acurácia de tal método, comparando o vetor de rótulos corretos com os novos gerados pelo classificador NN. A função para o cálculo da acurácia segue abaixo.

.accuracy
[source,python]
----
def accuracy(labels,prediction):
    count=0
    for i in range(0,labels.shape[0]):
        if(labels[i]==prediction[i]):
            count=count+1
    return (count/len(labels))*100
----

*_Rocchio:_*

Esta técnica também se baseia na distância Euclidiana, porém medida entre a amostra de teste e os centróides gerados pelas amostras de treino de mesmo rótulo ( à partir da média de seus atributos).

A função _split_by_labels_  recebe as amostras de treinamento e as separa em 3 grupos de acordo com seus rótulos.

.split_by_labels
[source,python]
----
def split_by_labels(array):
    L1=array[array[:,array.shape[1]-1]==1]
    L2=array[array[:,array.shape[1]-1]==2]
    L3=array[array[:,array.shape[1]-1]==3]
    return L1,L2,L3
----

Em seguida, utilizou-se a função _sample_centrois_ para calcular os centroides de cada rótulo de forma individual à partir da média de cada um dos atributos das amostras de mesmo rótulo.

.sample_centroids
[source,python]
----
def sample_centroids(data):
    m=np.empty((data.shape[1] -1),dtype=float)
    for i in range (0,data.shape[1]-1):
        m[i]=np.mean(data[:,i],dtype=np.float64)
    return m
----

Uma vez tendo as coordenadas de cada um dos centroids, implementou-se a função _Rocchio_ , que calcula a distância de cada amostra de teste e atualiza seu rótulo com o centróid que está mais próximo dela.

.Rocchio
[source,python]
----
def rocchio(data,Center_1,Center_2,Center_3):
    labels=[]
    euclidian=np.empty((3),dtype = float)
    for i in range(0,data.shape[0]):
        for j in range(0,(data.shape[1]-1)):
            euclidian[0]=np.sqrt(np.power(np.sum(Center_1[j]-data[i,j]),2))
            euclidian[1]=np.sqrt(np.power(np.sum(Center_2[j]-data[i,j]),2))
            euclidian[2]=np.sqrt(np.power(np.sum(Center_3[j]-data[i,j]),2))
            
        if(np.min(euclidian)==euclidian[0]):
            labels.append(1)
        elif(np.min(euclidian)==euclidian[1]):
            labels.append(2)
        else:
            labels.append(3)
    return labels
----

Ao final, calculou-se a acurácia deste algorítmo e obteve-se o seguinte resultado, exposto na Tabela 1:

.Resultados NN e Rocchio: questão a.
[%header,cols=2*] 
|===
|Algoritmo
|Acurácia

|NN
|76.00%

|Rocchio
|96.00%
|===

=== Solução 1.b ===

Para esta questão utilizou-se o arredondamento dos dados como solução para o ruído, transformando números decimais em números inteiros. Dando continuidade ao pre-processamento de dados que foi feito na letra a, utilizou-se o artifício de substituição de valores fora da faixa (outliers) pela moda do atributo no qual o mesmo se encontra, implementação realizada na função _replace_by_mode_. 

.replace_by_mode
[source,python]
----
def replace_by_mode(array):
    for i in range(0,array.shape[1]-1):
        while (i==0):
            index = np.where((array[:,i]<1) | (array[:,i]>3)) #get indexes
            index = index[0] #array of indexes
            m=stats.mode(array[:,i],axis=None) #calculating the mode of this atribute
            for j in range(0,len(index)):
                array[index[j],i]=int(m[0])
            i=1
        
        index = np.where((array[:,i]<1) | (array[:,i]>4))
        index = index[0]
        m=stats.mode(array[:,i],axis=None)
        for j in range(0,len(index)):
            array[index[j],i]=int(m[0])
    return array
----

Os resultados obtidos com esse tipo de abordagem estão expostos na Tabela 2:


.Resultados NN e Rocchio: questão b
[%header,cols=2*] 
|===
|Algoritmo
|Acurácia

|NN
|72.00%

|Rocchio
|100.00%
|===

=== Solução 1.c  ===

Comparando as Tabelas 1 e 2, nota-se para essa base de dados, o algoritmo Rocchio teve uma acurácia melhor do que o NN nos dois casos. O pre-processamento dos dados conseguiu otimizar a tarefa de classificação para o algoritmo Rocchio, trazendo um resultado de 100%, após a detecção e substituição dos outiliers e a eliminação do ruído.

*Arquivos*

|| link:01/01.py[01.py]  || link:01/functions01.py[functions01.py] ||

=== Questão 2 ===
Dada a base de dados Breast Cancer Wisconsin (Diagnostic) (baixar em
https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)):

a) Obtenha a acurácia de classificação quando usando o classificador vizinho mais
próximo (NN) (utilize a distância Euclidiana). Use os dados do arquivo wdbc.data,
sendo as primeiras 300 amostras para treino e as demais para teste. Antes, repare os
atributos da base de dados e a posição dos rótulos. Quais atributos você pode eliminar
da base de dados antes do experimento? Por quê?

b) Aplique o PCA sobre os dados de treino e selecione o número de componentes até eles
corresponderem a 90% da informação de variância dos dados (conforme mostrado nos
slides). Quantos componentes foram selecionados? Calcule a nova acurácia do NN
usando as componentes selecionadas. O resultado alterou de forma significativa em
relação ao obtido em a)? Qual foi a vantagem observada usando PCA?

c) Outra técnica para redução de dimensionalidade, mas de forma supervisionada, é o
Discriminante Linear de Fisher (para duas classes) e a sua versão multiclasse. Quando
aplicado este método o tamanho do vetor de características é reduzido para C-1, onde
C é o número de classes do problema. Seguindo os slides de
http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture8.pdf (há um exemplo
no meio), obtenha os novos dados após a aplicação de Fisher sobre os dados de treinoe obtenha a acurácia do NN sobre o conjunto de teste. Quais as vantagens desta abordagem sobre o PCA?

=== Solução 2.a  ===

Foram excluídas as duas primeiras colunas, pois elas dizem respeito ao ID e ao diagnóstico, variável alvo. Dessa forma,  se dividiu a base de dados conforme o enunciado e aplicou-se o algorítmo NN para a tarefa de classificação. Obtendo acurácia de *88.48%*.

=== Solução 2.b  ===

Para essa questão utilizou-se o cálculo da matriz de coeficientes de correlação, dessa forma o primeiro algoritmo implementado foi o para calcular a matriz stem:[\hat{X}], descrito abaixo:

[stem] 
++++ 
\hat{X}_{i,j}=\frac{X_{i,j}-\mu_{j}}{\sigma_{j}}
++++

Em seguida, se calculou a matriz de coeficientes de correlação stem:[C], com stem:[N=300]:

[stem]
++++
C=\frac{1}{(N-1)}\hat{X}^{T}\hat{X}
++++

O próximo passo foi extrair os autovalores e autovetores da matriz stem:[C]. Para tal, utilizou-se a função https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.eigh.html[numpy.linalg.eigh]. Para se encontrar o número de componentes que correspondem a *90%* da informação da variância dos dados, criou-se a função _reduce_percent_ , que soma os autovalores de forma decrescente até que o somatório relativo seja equivalente à condição de parada fornecida  pelo usuário (*90%*).

.reduce_percent
[source,python]
----
def reduce_percent(array,stop_condititon):
    total=0
    full=np.sum(array)
    for i in range(array.size-1,1,-1):
        total=total+(total+array[i])/full
        #print(total)
        if(total >= stop_condititon):
            return (total,abs(i-array.size))
----

Para essa base de dados, foram encontradas 5 componentes principais. Dessa forma, cria-se uma matriz com os autovetores associados aos 5 maiores autovalores encontrados (stem:[V_{30x5}]). Para se encontrar a nova matriz de amostras de treinamento com as 5 componentes principais,basta multiplicar a matriz stem:[\hat{X}_{300x30}] pela matriz stem:[V_{30x5}], resultando na matriz stem:[Mtr_{300x5}].

Para se encontrar a matriz de teste com as 5 principais componentes, basta se repetir o processo, porém utilizando o stem:[\mu] e stem:[\sigma] dos atributos da base de dados de treino.

A acurácia para a tarefa de classificação através do algorítmo NN com o conjunto de dados novo teve o valor de *72.12%*, uma diferença de *16.43%* do resultado para o conjunto de dados original.É importante ressaltar que o algoritmo PCA reduziu a dimensionalidade desse conjunto de dados em aproximadamente *6 vezes*, mantendo parte da informação contida nos dados originais (reduzindo-a em apenas *10%*), e tendo um resultado satisfatório.

*Arquivos*

|| link:01/02a.py[02a.py]  || link:01/02b.py[02b.py] || link:01/functions02.py[functions02.py] ||
 
=== Regressão Linear ===

=== Questão 3 ===
Para a base de dados link:01/Runner_num.txt[Runner] obtenha:

a) A equação linear que se ajusta aos dados e a RMSE;

b) Predizer o resultado para o ano de 2020;

c) Utilize o teste de hipótese de Kendall para verificar se existe dependência entre os atributos. Realize o teste para 5% e 1% de nível de significância. Informe os resultados;

d) Calcule o coeficiente de correlação entre os dados e realize o teste de hipótese de Pearson para 5% e 1% de nível de significância (teste bilateral). Informe os resultados.

=== Solução 3.a  ===

Inicialmente observou-se que a distribuição dos dados se assemelha a uma reta, polinômio de primeiro grau, conforme a Figura 1:


.Tempo dos corredores dos 100 metros livres das olimpíadas
image::03a_1.svg[a,600,opts=inline]

Dessa forma, realizou-se a regressão linear de primeiro grau para a base de dados. Os cálculos para encontrar os coefienteces stem:[w_{0}] e stem:[w_{1}] seguem abaixo:

[stem]
++++
w_{1} = \frac{\bar{xt}-\bar{x}\bar{t}}{\bar{x^{2}}-(\bar{x})^{2}}
++++

[stem]
++++
w_{0}=\bar{t}-w_{1}\bar{x}
++++

A equação de regressão linear que se ajusta ao conjunto de dados tem a seguinte forma e está ilustrada na Figura 2:

[stem]
++++
f=35.56 -0.012x
++++

.Equação de regressão linear stem:[f=35.56 -0.012x]
image::03a_2.svg[a,600,opts=inline]

Para calcular o erro RMSE utilizou-se a seguinte equação:

[stem]
++++
RMSE = \sqrt{\frac{1}{N}\sum_{k=1}^{N} (t_{k}-f(x_{k}))^{2}}
++++

O resultado obtido foi : RMSE = *22.13%*

=== Solução 3.b ===

Para se predizer o resultado para o ano de 2020, simplesmente se substituiu o valor 2020 na equação:

[stem]
++++
f(2020)=35.56 -0.012(2020)
++++

O resultado obtido foi: f(2020) = 9.53 s

=== Solução 3.c ===

Para o cálculo do stem:[\tau], utiliza-se a seguinte fórmula:

[stem]

++++
\tau = \frac{\sum_{i=2}^{N} \sum_{j=1}^{i-1} sgn(x_{j}-x_{i})sgn(y_{j}-y_{i})}{\sqrt{n_{x}}\sqrt{n_{y}}}
++++

Tal fórmula foi implementada com o auxílio da função  https://docs.python.org/2/library/itertools.html[itertools] para  criar uma lista com combinações entre os índices dos dois arrays. Em seguida criou-se a função _get_tal_ para calcular o stem:[\tau] .

.get_tal
[source,python]
----
def get_tal(a1,a2,combinations):
    positivos = 0
    negativos = 0
    matrix = 0
    for i in range(combinations.shape[0]):
        matrix=(a1[combinations[i][0]]-a1[combinations[i][1]])*(a2[combinations[i][0]]-a2[combinations[i][1]])
        if(matrix>0):
            positivos = positivos +1
        else:
            negativos=negativos+1
   
    N=(a1.size*(a1.size-1))/2

    tal = (positivos-negativos)/(N)
    return tal
----

O valor de stem:[\tau] calculado foi de : stem:[|\tau|] = 0.87. Pela tabela de distribuição de  T de Student: stem:[z_{1-\frac{0.05}{2}}=1.96] e stem:[z_{1-\frac{0.01}{2}}= 2.33]. Substituindo seus valores na equação para o teste de hipótese dos coeficientes de correlação de Kendall, tem-se:

Para 5%:
[stem]
++++
|\tau| > 1.96 \sqrt{\frac{2(2N+5)}{9N(N-1)}} 

\rightarrow 0.87 > 0.26
++++

Portanto, a hipótese nula foi rejeitada para 5% e existe a possibilidade de haver dependência entre x e y com 95% de significância.

Já para 1%:

[stem]
++++
|\tau| > 2.33 \sqrt{\frac{2(2N+5)}{9N(N-1)}} 

\rightarrow 0.87 > 0.31
++++

Portanto, a hipótese nula foi rejeitada para 1% e existe a possibilidade de haver dependência entre x e y com 99% de significância.


=== Solução 3.d ===

Para realizar o teste de hipótese pelo coeficiente de correlação de Pearson, primeiramente foi obtido o valor P, coeficietne de correlação entre dois atributos, utilizando-se a seguinte fórmula:

[stem]
++++
P=\frac{cov(x,y)}{\sqrt(var(x))\sqrt(var(y))} \rightarrow P = -0.91
++++

Nota-se que o valor de P se encontra próximo ao valor -1, o que significa que a correlação linear é quase perfeita negativa entre x e y.

Em seguida, calcula-se a estatística do teste:

[stem]
++++
t_{0}=\frac{\hat{p}\sqrt{N-2}}{\sqrt(1-\hat{p}^{2})} \rightarrow t_{0}= -11.26
++++

Substituindo seu valor e os valores de stem:[t_{\frac{0.05}{2},N-2} =2.052] e stem:[t_{\frac{0.01}{2},N-2} =2.771] fórmula para hipótese nula rejeita, tem-se:

Para 5%:

[stem]
++++
|t_{0}|>2.052 \rightarrow 11.26 > 2.052
++++

A hipótese nula foi rejeitada para 5%, portanto há confiabilidade de 95%.

Para 1%:

[stem]
++++
|t_{0}|>2.771 \rightarrow 11.26 > 2.771
++++

A hipótese nula foi rejeitada para 5%, portanto há confiabilidade de 99%.

*Arquivos*

|| link:01/03.py[03.py]  ||

=== Questão 4 === 
Para a base de dados Auto MPG (disponibilizada em https://archive.ics.uci.edu/ml/datasets/Auto+MPG) faça:

a) Baixe o arquivo auto-mpg.data, remova as linhas que tem interrogação (?) e remova a última coluna (por quê?). Com as 150 primeiras linhas obtenha um modelo de regressão linear multivariada para predizer o valor da primeira variável (mpg). Avalie o resultado sobre o restante da base de dados, usando a métrica RMSE.

b) Verifique quais são os atributos que estão relacionados com a saída: A partir dos coeficientes obtidos, aplique o teste F de Snedecor sobre cada variável individualmente (conforme nos slides). Indique quais foram os atributos que podem ser desconsiderados. Obtenha sobre o restante da base de dados a métrica RMSE com o modelo sem considerar esses atributos (não precisa estimar um novo modelo, só considere os valores dos coeficientes deles iguais a zero). Compare os resultados obtidos em a) e em b). Considere que os resíduos do modelo possui distribuição aproximadamente normal e que stem:[F_{1,142} = 3,908].

=== Solução 4.a ===

As amostras com dados faltantes ("?") foram excluídas e o último atributo também, já que não apresenta nenhuma informação relevante para a tarefa de classificação, pois o modelo do carro não intefere em suas características. 

A rotina para a regressão foi elaborada por meio matricial, sendo possível estimar a matriz de coeficientes “W”. De posse dessa matriz foi possível calcular o vetor de saídas desejadas “t”.
 
Ao se aplicar a métrica RMSE sobre o conjunto de dados de teste foi encontrado o resultado de *6.247*.Tendo em vista que o RMSE penaliza grandes desvios entre o valor real e o estimado,para essa base de testes ele demonstrou um bom ajuste dos
dados ao modelo proposto.

=== Solução 4.b ===

Para saber se um atributo (ou conjunto de atributos) está contribuindo ou não ao modelo é
necessário se fazer o teste estatístico entre o modelo obtido com e sem o atributo analisado (teste F de
Snedecor). 

Para a elaboração desse teste, primeiro foi feito o modelo com todos os atributos e depois, cada atributo era eliminado, por vez, e calculava novamente o modelo. De posse dos valores encontrados é possível comparar qual ou quais atributos podem ser descartados. Esta
comparação é feita com o valor de 3,908, referente a distribuição normal dos dados. Sendo assim, os atributos eliminados foram os atributos 3 (x2) e 6 (x5), displacement e acceleration respectivamente.

Após a eliminação dos atributos selecionados, o novo valor de RMSE foi de 6,099, um valor
ligeiramente abaixo dos 6,247 apurados, utilizando todos os atributos. Esse resultado
confirma que os dois atributos eliminados não estavam contribuindo para a classificação.


*Arquivos*

|| link:01/04.py[04.py]  ||


=== Questões Teóricas === 

=== Questão 1 ===

Explique o dilema entre bias e variância e o seu relacionamento com underfitting e
overfitting.
=== Solução Questão 1 ===

Bias pode ser definido como a diferença entre o valor esperado calculado através do modelo desenvolvido e  o real na qual. Ele está diretamente associado à habilidade que o modelo desenvolvido tem em se ajustar conjunto de dados. Um modelo pode ser dito com um alto bias quando a sua estrutura não descreve corretamente os dados. Em resumo, Bias é o erro que ocorre ao tentar aproximar o comportamento dos dados.

Quando um modelo tem uma alta variância ele representa muito bem os dados, porém ao se trocar o conjunto de dados é comum que o resultado não seja satisfatório para a predição. A variância está associada com a quantidade na qual o modelo será alterado conforme um conjunto diferente.

O Bias, nos permite entender o comportamento do modelo e nos dá a possibilidade de tomar ações corretivas. Quando o modelo tem um alto bias significa que pode-se adicionar mais atributos para melhorar a tarefa de classificação. No caso de alta variância, uma alternativa é redução de atributos, ou inclur mais amostras.

=== Questão 2 ===

Comente sobre a veracidade das afirmações:

a)“Quanto mais variáveis de entrada forem usadas em um modelo de aprendizado de
máquina, melhor será a qualidade do modelo”.

b)“Independente da qualidade, quanto mais amostras forem obtidas para uma base de
dados, maior a tendência de se obter modelos mais adequados”.

c)“Às vezes com simples manipulações na base de dados (limpeza, conversão de valores, etc.) pode-se conseguir melhoras significativas nos resultados, sem fazer nenhuma alteração na técnica de aprendizado de máquina usada”.

=== Solução 2.a === 

A afirmação está errada. Isto pode ser observado na questão 02.b. O conjunto inicial possui 30 atributos. Aplicou-se o PCA sobre o esse conjunto e obteve-se a mesma acurária próxima ao do conjunto original na questão 02.a. O importante é se reduzir o conjunto original, porém mantendo a relação entre as amostras.

=== Solução 2.b === 

A afirmação está errada, pois não se basta ter um grande número de amostras sem nenhum tipo de pré-processamento. Ao não se aplicar um pré-processamento no conjunto de dados, se corre o grande risco de realizar tarefas de classificação ambíguas, pois não se garante a qualidade dos dados.

=== Solução 2.c === 

A afirmação está correta. O pré-processamento de dados tem a capacidade de otimizar o procedimento de classificação e garantir que não haverá resultado distorcido.

=== Questão 3 ===

Em certas tarefas de aprendizado supervisionado as amostras de diferentes classes aparecem com sobreposição, de tal forma que não é possível obter uma superfície que separe de forma adequada as amostras das diferentes classes. O que se poderia fazer nestas situações para tentar melhorar a qualidade de classificação?

=== Solução Questão 3 ===

É possível realizar testes sobre a relação linear dos atributos para saber quais realmente influenciam ou não no comportamento dos dados com a finalidade de remover algum atributo que possa estar contribuindo diretamente para essa situação.

=== Questão 4 ===

Quais devem ser as características que uma base de dados deve ter para:

a) Uma regressão linear se ajustar bem aos dados?
b) O classificador Rocchio conseguir um bom resultado de classificação?
c) O classificador Vizinho mais Próximo conseguir um bom resultado de classificação?

=== Solução 4.a ===

Haver uma boa identificação do polinômio interpolador, baseado na observação dos dados, e que seja comprovada, através de testes de hipótese,
a interdependência dos atributos.

=== Solução 4.b ===

Uma base de dados com todas as classes balanceadas igualmente.

=== Questão 5 ===

Em uma empresa é adotado um método de Aprendizado de Máquina para detectar
defeito de fabricação de peças mecânicas, sendo que raramente acontece este tipo de
problema na fábrica. Um funcionário anuncia empolgado que o sistema alcançou uma
acurácia de 99%, porém seu gerente não achou o resultado tão relevante. Responda:

a) Por que o gerente não ficou empolgado com o resultado achado?

b) O que o funcionário poderia fazer para confirmar se o método empregado é adequado
para o problema?

=== Solução 5.a ===

Pois, conforme o enunciado cita, há um número baixo de amostras classificadas como defeituosas em relação ao número de amostras sem defeito. Dessa forma, o conjunto de dados está desbalanceado tendo como resultado favorável a classe majoritária.

=== Solução 5.b ===



Reduzir o número de amostras sem defeito, com a finalidade de garantir o correto balanceamento dos dados.



== Lista 2  ==

=== Métodos de Classificação Baseados em Probabilidade ===

=== Questão 1 ===
Para a base Car Evaluation (disponível em http://archive.ics.uci.edu/ml/), considerando que o primeiro atributo é x1, o segundo é x2 e assim por diante, estime as probabilidades:

a)P(x1 =med) e P(x2 = low)

b)P(x6=high|x3=2) e P(x2=low|x4=4)

c)P(x1=low|x2=low,X5=small) e P(x4=4|x1=med,X3=2)

d)P(x2= vhigh,X3=2|X4=2) e P(x3=4,x5=med|x1=med)



=== Solução 1.a ===
A probabilidade calculada para ambas foi de 25 stem:[\%].

=== Solução 1.b ===
O resultado obtido para *stem:[P(x6=high|x3=2)]* foi de stem:[33.33\%], já para *stem:[P(x2=low|x4=4)]* foi de stem:[25\%].

=== Solução 1.c ===
O resultado obtido para *stem:[P(x1=low|x2=low,X5=small)]* foi de stem:[25.00\%], já para *stem:[P(x4=4|x1=med,X3=2)]* foi de stem:[33.33\%].

=== Solução 1.d ===
O resultado obtido para *stem:[P(x2=vhigh,X3=2|X4=2)]* foi de stem:[6.25\%], já para *stem:[P(x3=4,x5=med|x1=med)]* foi de stem:[8.33\%].


=== Questão 2 ===
Aplique o Naive Bayes sobre a base de dados Monk's Problems (disponível em http://archive.ics.uci.edu/ml/). Obtenha a acurácia, treinando com monks-2.train e testando em monks-2.test. Realize os experimentos:

a) Considerando uma distribuição Gaussiana dos atributos;
b)Discretizando os valores em intervalos de tamanho 1;
c)Discretize os valore da mesma forma que em b) usando a suavização de Laplace.


=== Questão 3 === 

Para a rede bayesiana da figura abaixo, verifique as seguintes afirmações,
indicando se é falso ou verdadeiro e fornecendo a devida explicação.

image::a.png[250,250,align="center"]

a) A é independe de B

b)A é independe de B tal que foi observado G

c)C é independe de B tal que foi observado H

d)G é independente de E tal que foi observado B e I

e)G é independente de I tal que foi observado A e B



=== Solução 3.a ===
stem:[A \rightarrow I \leftarrow B]: caminho bloqueado, pois I não é conhecido.

stem:[A \rightarrow D \rightarrow G \leftarrow B]:  caminho bloqueado, pois G não é conhecido.

stem:[A \rightarrow C \rightarrow G \leftarrow B]:  caminho bloqueado, pois G não é conhecido.

Dessa forma, A e B são independentes.


=== Solução 3.b ===

Analizando os caminhos que levam de A até B, tem-se:

stem:[A \rightarrow I \leftarrow B]: caminho bloqueado, pois I não é conhecido.

stem:[A \rightarrow D \rightarrow G \leftarrow B]:  caminho desbloqueado, pois G é conhecido.

stem:[A \rightarrow C \rightarrow G \leftarrow B]:  caminho desbloqueado, pois G é conhecido.

Dessa forma, A e B não são independentes.



=== Solução 3.c === 
Analizando os caminhos que levam de C até B, tem-se:


stem:[C \leftarrow A \rightarrow I \leftarrow B]: caminho bloqueado, pois I não é conhecido, logo C e B são bloqueados.

stem:[C \rightarrow G \leftarrow B]:  caminho bloqueado, pois G não é conhecido.

Portanto, C e B são independentes.


=== Solução 3.d ===
Analizando os caminhos que levam de D até E, tem-se:

stem:[G \leftarrow B \rightarrow E]: caminho bloqueado, pois B é conhecido, logo G e E são bloqueados nesse caminho.

stem:[G \leftarrow D \leftarrow A \rightarrow I \leftarrow B \rightarrow E]:  caminho bloqueado, pois B é conhecido, logo G e E são bloqueados nesse caminho.

stem:[G \leftarrow D \leftarrow A \rightarrow I \leftarrow H \leftarrow E]:  caminho desbloqueado, pois H não é conhecido, logo G e E não estão bloqueados nesse caminho.


Portanto, G e E não são independentes, pois nem todos os caminhos estão bloqueados.


=== Solução 3.e ===
Analizando os caminhos que levam de G até I, tem-se:
 
stem:[G \leftarrow B \rightarrow I]: Caminho entre G e I bloqueado, pois B é conhecido.

stem:[G \leftarrow D \leftarrow A \rightarrow I]: G e I são bloqueados, pois A é conhecido.

Dessa forma, G e I são independentes.

=== Questão 4 ===

Dada a rede bayesiana abaixo e a base de dados trânsito.txt, obtenha (na base de dados os valores 0 indicam que o evento não aconteceu, enquanto 1 aconteceu, sendo C chuva, F feriado, E ngarrafamento, R ruas alagadas e A acidente):

image::b.png[250,250,align="center"]

a)As probabilidades condicionais e a priori de cada nó necessários para o uso da
rede.

b)A probabilidade de acontecer acidente tal que foi observado ruas alagadas
(P(Acidente = 1|Ruas alafgadas = 1))

c)A probabilidade de ser feriado tal que foi observado chuva e engarrafamento
(P(Feriado = 1|Chuva = 1, Engarrafamento = 1))

=== Solução 4.a ===


stem:[P(C) = 47 \%]

stem:[P(F) = 22 \%]

stem:[\frac{P(C)}{P(R = 0, C=0)} = 0.69 \%]

stem:[\frac{P(C)}{P(R = 1, C=0)} = 99.31 \%]

stem:[\frac{P(C)}{P(R = 1, C=1)} = 99.22 \%]

stem:[\frac{P(C)}{P(R = 0, C=1)} = 0.78 \%]


stem:[R  = 0, E = 0 , A = 0: 0.87 \%]

stem:[R  = 0, E = 0 , A = 1: 99.13 \%]

stem:[R  = 0, E = 1 , A = 0: 0.85 \%]

stem:[R  = 0, E = 1 , A = 1: 99.14 \%]

stem:[R  = 1, E = 0 , A = 0: 0.25 \%]

stem:[R  = 1, E = 0 , A = 1: 99.75 \%]

stem:[R  = 1, E = 1 , A = 0: 0.14 \%]

stem:[R  = 1, E = 1 , A = 1: 99.86 \%]

=== Solução 4.b ===

stem:[P(A = 1|R  = 1) = 83.33 \%]

=== Solução 4.c ===

stem:[P(F = 1|C = 1, E = 1) = 64.71 \%]

